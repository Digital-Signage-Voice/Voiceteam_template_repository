# main_pipeline.py (v2 - Updated by Haechan)
# ì—­í• : ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì´ê´„í•˜ë©°, ê° íŒ€ì›ì˜ 'ì™„ì„±ëœ ëª¨ë“ˆ'ì„ ê°€ì ¸ì™€ ì¡°ë¦½í•˜ëŠ” ì§€íœ˜ì

import cv2
import numpy as np
from moviepy.editor import VideoFileClip
import soundfile as sf

# --- ëª¨ë“ˆ í†µí•© ì§€ì  ---
# í˜„ì§€ë‹˜ì˜ 'ì˜ìƒ ì²˜ë¦¬ ì—”ì§„' í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
# (ì´ì œ video_module.py ëŒ€ì‹ , í˜„ì§€ë‹˜ì´ ë§Œë“  processor.pyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)
from src.video.processor import VideoProcessor

# ì›í›„ë‹˜ì˜ 'ìŒì„± ì²˜ë¦¬ ì—”ì§„' í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (ê¸°ì¡´ê³¼ ë™ì¼)
from src.audio.audio_module import denoise_audio


def run_voice_team_pipeline(input_video_path: str, output_audio_path: str):
    """
    ìš°ë¦¬ VoiceíŒ€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    print("ğŸš€ VoiceíŒ€ ì „ì²´ íŒŒì´í”„ë¼ì¸ v2ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")

    # --- 1ë‹¨ê³„: ëª¨ë“ˆ ì¤€ë¹„ ---
    # í˜„ì§€ë‹˜ì˜ ì˜ìƒ ì²˜ë¦¬ ì—”ì§„ì„ ìƒì„±í•©ë‹ˆë‹¤. (ì˜¤í”„ë¼ì¸ ì˜ìƒ íŒŒì¼ ëª¨ë“œ)
    print("ğŸ› ï¸  1ë‹¨ê³„: í˜„ì§€ë‹˜ì˜ ì˜ìƒ ì²˜ë¦¬ ì—”ì§„ì„ ì¤€ë¹„í•©ë‹ˆë‹¤...")
    # source='video'ë¡œ ì„¤ì •í•˜ì—¬ íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ê²Œ í•©ë‹ˆë‹¤.
    video_processor = VideoProcessor(source='video', path=input_video_path, visualize=False)

    # --- 2ë‹¨ê³„: ì¬ë£Œ ì¤€ë¹„ ---
    print("ğŸ”ª 2ë‹¨ê³„: ë™ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
    video_clip = VideoFileClip(input_video_path)
    full_audio_data = video_clip.audio.to_soundarray()
    sample_rate = video_clip.audio.fps
    final_audio_data = full_audio_data.copy()

    # --- 3ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í”„ë ˆì„ ë‹¨ìœ„ ì²˜ë¦¬) ---
print("\nğŸ”„ 3ë‹¨ê³„: ì˜ìƒì˜ ê° í”„ë ˆì„ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ë©° íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")

cap = cv2.VideoCapture(input_video_path)
frame_id = 0
fps = cap.get(cv2.CAP_PROP_FPS)  # ì˜ìƒì˜ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜
frame_duration = 1 / fps        # í•œ í”„ë ˆì„ì´ ì°¨ì§€í•˜ëŠ” ì‹œê°„ (ì´ˆ)

processed_audio_segments = []  # ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì¡°ê°ë“¤ì„ ëª¨ì•„ë‘˜ ë¦¬ìŠ¤íŠ¸

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # 1. í˜„ì§€ë‹˜ ëª¨ë“ˆë¡œ ì˜ìƒ í”„ë ˆì„ ë¶„ì„
    result_dict = video_processor.process_frame(frame_id, frame)
    is_speaking = result_dict['is_speaking']
    timestamp = result_dict['timestamp']

    print(f"  - Frame #{frame_id}: í˜„ì¬ ì‹œê°„ {timestamp:.2f}ì´ˆ, ë°œí™” ì—¬ë¶€: {is_speaking}")

    # 2. íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ìš©í•´ í˜„ì¬ í”„ë ˆì„ì— í•´ë‹¹í•˜ëŠ” ì˜¤ë””ì˜¤ ì¡°ê° ì¶”ì¶œ
    start_sample = int(timestamp * sample_rate)
    end_sample = int((timestamp + frame_duration) * sample_rate)
    audio_chunk = full_audio_data[start_sample:end_sample]

    # 3. âœ¨ í•µì‹¬ í†µí•© ë¡œì§ âœ¨
    if is_speaking:
        # 'ë°œí™” ì¤‘'ì´ë©´ ì›í›„ë‹˜ ëª¨ë“ˆë¡œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ì˜ˆ: ë…¸ì´ì¦ˆ ì œê±°)
        processed_chunk = denoise_audio(audio_chunk, sample_rate)
        processed_audio_segments.append(processed_chunk)
    else:
        # 'ë¹„ë°œí™” ì¤‘'ì´ë©´ ì†Œë¦¬ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ Mute ì²˜ë¦¬
        silence_chunk = np.zeros_like(audio_chunk)
        processed_audio_segments.append(silence_chunk)

    frame_id += 1

cap.release()
print("\nâœ… ì˜ìƒì˜ ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 4ë‹¨ê³„: í›„ì²˜ë¦¬ ë° ê²°ê³¼ë¬¼ ì €ì¥ ---
print("\nğŸ’¾ 4ë‹¨ê³„: ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì¡°ê°ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")

# ëª¨ë“  ì˜¤ë””ì˜¤ ì¡°ê°ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
final_audio_data = np.concatenate(processed_audio_segments)

sf.write(output_audio_path, final_audio_data, sample_rate)
print(f"ğŸ‰ ì„±ê³µ! íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ê²½ë¡œ: '{output_audio_path}')")
